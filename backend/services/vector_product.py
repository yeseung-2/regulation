import pandas as pd
from pathlib import Path
from langchain.docstore.document import Document
from langchain_openai import OpenAIEmbeddings  # âœ… ìµœì‹  ë²„ì „
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv
import os

# âœ… .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# âœ… ê²½ë¡œ ì„¤ì •
CHUNK_PATH = Path("sasb_chunks.xlsx")  # sasbìš© ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
VECTORSTORE_PATH = Path("vectorstores/sasb")  # ì €ì¥í•  ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ

# âœ… ì—‘ì…€ ë¡œë“œ
print("ğŸ“„ ì²­í¬ ë¯¸ë¦¬ë³´ê¸° ë¡œë”© ì¤‘...")
df = pd.read_excel(CHUNK_PATH)

# âœ… LangChain ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜
documents = []
for _, row in df.iterrows():
    if pd.isna(row["text"]):
        continue

    metadata = {
        "chunk_id": row["chunk_id"],
        "title": row["title"],
        "pages": row["pages"],
        "tables": row["tables"],
        "images": row["images"]
    }
    documents.append(Document(page_content=str(row["text"]), metadata=metadata))

# âœ… ì„ë² ë”© ëª¨ë¸ ì„¤ì •
print("ğŸ” ì„ë² ë”© ìƒì„± ì¤‘...")
embedding = OpenAIEmbeddings()

# âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ë¡œ í† í° ì´ˆê³¼ ë°©ì§€)
def batch(iterable, n=1000):
    for i in range(0, len(iterable), n):
        yield iterable[i:i + n]

vectorstore = None
for i, doc_batch in enumerate(batch(documents, n=100)):
    print(f"ğŸ”¢ ë°°ì¹˜ {i+1} ì²˜ë¦¬ ì¤‘... ({len(doc_batch)}ê°œ ì²­í¬)")
    if vectorstore is None:
        vectorstore = FAISS.from_documents(doc_batch, embedding)
    else:
        new_store = FAISS.from_documents(doc_batch, embedding)
        vectorstore.merge_from(new_store)

# âœ… ì €ì¥
print("ğŸ’¾ FAISS ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì¤‘...")
VECTORSTORE_PATH.mkdir(parents=True, exist_ok=True)
vectorstore.save_local(str(VECTORSTORE_PATH))
print(f"âœ… ì €ì¥ ì™„ë£Œ: {VECTORSTORE_PATH}/index.faiss")
