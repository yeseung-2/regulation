from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI
from langchain.schema import Document, SystemMessage, HumanMessage, AIMessage
from services.vector_loader import load_vectorstore
from key import key
import json
import re
from pathlib import Path
from sentence_transformers import SentenceTransformer, util
from bs4 import BeautifulSoup
import torch

# âœ… ë²ˆì—­ ìºì‹œìš©
CACHE_PATH = Path("translation_cache.json")
translation_cache = {"ko2en": {}, "en2ko": {}}

model = SentenceTransformer("all-MiniLM-L6-v2")

def load_cache():
    global translation_cache
    if CACHE_PATH.exists():
        with open(CACHE_PATH, "r", encoding="utf-8") as f:
            translation_cache = json.load(f)

def save_cache():
    with open(CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(translation_cache, f, ensure_ascii=False, indent=2)

def extract_clean_table_html(html_text: str) -> str:
    soup = BeautifulSoup(html_text, "html.parser")
    h3 = soup.find("h3")
    table = soup.find("table")
    if h3 and table:
        return f"{str(h3)}\n{str(table)}"
    return ""

# âœ… GPT ë²ˆì—­ with ìºì‹±
def translate_to_english(text: str) -> str:
    if text in translation_cache["ko2en"]:
        return translation_cache["ko2en"][text]

    llm = ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=key["OPENAI_API_KEY"])
    prompt = [
        SystemMessage(content="Translate the following Korean text to English. Respond only with the translated English."),
        HumanMessage(content=text)
    ]
    translated = llm.invoke(prompt).content
    translation_cache["ko2en"][text] = translated
    return translated

def translate_to_korean(text: str) -> str:
    if text in translation_cache["en2ko"]:
        cached = translation_cache["en2ko"][text]
        if "í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤" in cached or "ë°ì´í„°" in cached:
            print(f"âŒ fallback ë²ˆì—­ ê°ì§€ë¨ â†’ ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬ë²ˆì—­í•©ë‹ˆë‹¤: {cached}")
        else:
            return cached

    llm = ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=key["OPENAI_API_KEY"])
    prompt = [
        SystemMessage(content="ë‹¤ìŒ ì˜ì–´ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì¤˜. ë°˜ë“œì‹œ ë²ˆì—­ëœ ë¬¸ì¥ë§Œ ì‘ë‹µí•´."),
        HumanMessage(content=text)
    ]
    translated = llm.invoke(prompt).content

    # ë‹¤ì‹œ í•œë²ˆ fallback íƒì§€ (ì‘ë‹µê¹Œì§€ ì˜ì‹¬ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆìŒ)
    if "í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤" in translated or "ë°ì´í„°" in translated:
        print("âŒ ì¬ë²ˆì—­ë„ fallback íƒì§€ë¨ â†’ ì‘ë‹µ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ")
        return "ì•„ë˜ëŠ” ìš”ì²­í•˜ì‹  GRI ì›ë¬¸ ë²ˆì—­ë³¸ ì…ë‹ˆë‹¤.\n\n" + text

    translation_cache["en2ko"][text] = translated
    return translated

def clean_translation_cache(fallback_keywords=None):
    fallback_keywords = fallback_keywords or ["í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤", "ë°ì´í„°", "2023ë…„", "model"]
    cleaned = False

    for direction in ["en2ko", "ko2en"]:
        original = dict(translation_cache[direction])  # ë³µì‚¬
        for k, v in original.items():
            if any(kw in v for kw in fallback_keywords):
                print(f"ğŸ§¹ ìºì‹œ ì œê±°ë¨ â†’ {k} â†’ {v}")
                del translation_cache[direction][k]
                cleaned = True

    if cleaned:
        save_cache()
        print("âœ… translation_cache.json ì •í™” ì™„ë£Œ ë° ì €ì¥ë¨")
    else:
        print("ğŸ§¼ ì •í™”í•  ìºì‹œ ì—†ìŒ")

# âœ… GPT ê¸°ë°˜ ì§ˆë¬¸ ë¶„ë¥˜
def classify_query(question: str) -> str:
    llm = ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=key["OPENAI_API_KEY"])
    system_prompt = """
ë„ˆëŠ” ESG ë¬¸ì„œë¥¼ ë‹¤ë£¨ëŠ” AI ë¬¸ì„œ ë¶„ë¥˜ê¸°ì•¼.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì½ê³ , ë°˜ë“œì‹œ ì•„ë˜ 4ê°€ì§€ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•´ì•¼ í•´. ë‹¤ë¥¸ ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆ.

ê°€ëŠ¥í•œ ê°’:
- esg_Manual: ê¸°ë³¸ì ì¸ ì§ˆë¬¸, ESG ë³´ê³ ì„œ ì‘ì„± ì˜ˆì‹œ, GRI ì§€í‘œì— ëŒ€í•œ ì¼ë°˜ ì„¤ëª…
- GRI_Standards: ì§ˆë¬¸ì— 'ì›ë¬¸'ì´ë¼ëŠ” ë‹¨ì–´ê°€ ìˆì„ ë•Œë§Œ ì„ íƒ (ì˜ˆ: "GRI 305-1 ì›ë¬¸ ë³´ì—¬ì¤˜")
- esg_templates: 'ê·œì •', 'ê·œì •ì•ˆ', 'ì§€ì¹¨', 'í…œí”Œë¦¿'ê³¼ ê°™ì€ ë¬¸êµ¬ê°€ ìˆìœ¼ë©´ ì„ íƒ
- esg_sample1: íŠ¹ì • ê¸°ì—…ì˜ ì‹¤ì œ ì‚¬ë¡€ë¥¼ ë¬¼ì„ ë•Œ ì„ íƒ (ì˜ˆ: "íŒŒë‚˜ì‹œì•„ì˜ ESG ê²½ì˜ ì‚¬ë¡€ ì•Œë ¤ì¤˜")

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´:
{ "index": "ì„ íƒê°’" }

ì˜ˆì‹œ:
ì§ˆë¬¸: "GRI 305-1 ì›ë¬¸ ì•Œë ¤ì¤˜"
ì‘ë‹µ: { "index": "GRI_Standards" }

ì§ˆë¬¸: "í™˜ê²½ê²½ì˜ ê·œì •ì•ˆ ì–‘ì‹ ì•Œë ¤ì¤˜"
ì‘ë‹µ: { "index": "esg_templates" }

ì§ˆë¬¸: "SKëŠ” ì–´ë–»ê²Œ ëŒ€ì‘í–ˆì–´?"
ì‘ë‹µ: { "index": "esg_sample1" }

ì§ˆë¬¸: "ì¤‘ëŒ€ì„± í‰ê°€ í•­ëª©ì€ ë­ì•¼?"
ì‘ë‹µ: { "index": "esg_Manual" }
"""
    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=question)
    ]).content

    try:
        index = json.loads(response)["index"]
        print(f"âœ… ì„ íƒëœ ë²¡í„°ìŠ¤í† ì–´ index_name: {index}")
        return index
    except Exception:
        print("âŒ GPT ë¶„ë¥˜ ì‹¤íŒ¨ â†’ ê¸°ë³¸ê°’ 'esg_manual'")
        return "esg_Manual"

# âœ… ë©”íƒ€ë°ì´í„°ì—ì„œ í‘œ/ì´ë¯¸ì§€ ì¶”ì¶œ
def extract_metadata(documents: list[Document]) -> dict:
    tables, images = [], []
    seen_table_paths = set()
    seen_image_paths = set()

    for doc in documents:
        # ğŸ“Œ í‘œ ì²˜ë¦¬
        t_raw = doc.metadata.get("tables", [])
        t = eval(t_raw) if isinstance(t_raw, str) else t_raw

        for table_path in t:
            if table_path in seen_table_paths:
                continue
            seen_table_paths.add(table_path)

            full_path = Path(table_path)
            if full_path.exists():
                table_html = full_path.read_text(encoding="utf-8")
                tables.append(table_html)

        # ğŸ“Œ ì´ë¯¸ì§€ ì²˜ë¦¬
        i_raw = doc.metadata.get("images", [])
        i = eval(i_raw) if isinstance(i_raw, str) else i_raw
        for img in i:
            if img not in seen_image_paths:
                seen_image_paths.add(img)
                images.append(img)

    return {
        "tables": tables,
        "images": images
    }

def extract_page_number_from_path(path: str) -> int | None:
    name = Path(path).stem
    if name.startswith("page") and "_" in name:
        return int(name.replace("page", "").split("_")[0])
    return None

def select_best_page(answer: str, table_paths: list[str], user_question: str = "") -> int | None:
    infos = []
    seen = set()
    for path in table_paths:
        if path in seen or not Path(path).exists():
            continue
        seen.add(path)

        soup = BeautifulSoup(Path(path).read_text(encoding="utf-8"), "html.parser")
        title = soup.find("h3").text.strip() if soup.find("h3") else ""
        headers = [th.get_text(strip=True) for th in soup.find_all("th")]
        cells = [td.get_text(strip=True) for td in soup.find_all("td")]
        content = " ".join(cells[:20])[:300]

        summary = f"{title}\n{', '.join(headers)}\n{content}"
        infos.append({"path": path, "summary": summary})

    if not infos:
        return None

    # âœ… ì§ˆë¬¸ë„ í¬í•¨
    query = f"{answer}\n\n{user_question}".strip()
    query_embedding = model.encode(query, convert_to_tensor=True)
    table_embeddings = [model.encode(i["summary"], convert_to_tensor=True) for i in infos]
    table_embeddings = torch.stack(table_embeddings)

    scores = util.cos_sim(query_embedding, table_embeddings)[0]
    best_idx = scores.argmax().item()

    best_path = infos[best_idx]["path"]
    page = extract_page_number_from_path(best_path)
    print(f"ğŸ“„ ìœ ì‚¬í•œ í‘œ ê²½ë¡œ: {best_path} â†’ page{page}")
    return page


# âœ… í˜ì´ì§€ ë‹¨ìœ„ ë¦¬ì†ŒìŠ¤ ë¡œë”©
def load_resources_for_page(base_dir: str, page: int) -> tuple[list[str], list[str]]:
    table_dir = Path(base_dir) / "tables_gpt"
    tables = [str(p) for p in table_dir.glob(f"page{page}_table*.html")]

    image_dir = Path(base_dir) / "images"
    images = [str(p) for p in image_dir.glob(f"*page{page}*") if p.suffix.lower() in [".png", ".jpg", ".jpeg"]]

    return tables, images

def generate_suggested_questions(context_snippet: str, user_question: str, index_name: str) -> list[str]:
    # ğŸ”’ íŠ¹ì • indexëŠ” ì§ˆë¬¸ ìƒì„± ì œì™¸
    if index_name in ["esg_templates", "esg_sample1"]:
        return []

    llm = ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=key["OPENAI_API_KEY"])
    prompt = [
        SystemMessage(content="""ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤.
ì´ ë‘˜ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìê°€ ì´ì–´ì„œ í•  ìˆ˜ ìˆëŠ” ESG ê´€ë ¨ ì‹¤ë¬´ ì§ˆë¬¸ì„ 3ê°œ ì¶”ì²œí•´ ì£¼ì„¸ìš”.
- ì§ˆë¬¸ì€ ê°„ê²°í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
- ì§ˆë¬¸ ì™¸ ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
"""),
        HumanMessage(content=f"""[ì‚¬ìš©ì ì§ˆë¬¸]
{user_question.strip()}

[ë¬¸ì„œ ë‚´ìš©]
{context_snippet.strip()}""")
    ]
    try:
        response = llm.invoke(prompt).content
        lines = [line.strip("-â€¢0123456789. ") for line in response.strip().split("\n") if line.strip()]
        return lines[:3]
    except Exception as e:
        print("âŒ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨:", e)
        return ["ì´ í•­ëª©ì— ëŒ€í•´ ìì„¸íˆ ì•Œë ¤ì¤˜ìš”.", "ê´€ë ¨ ì‚¬ë¡€ê°€ ìˆë‚˜ìš”?", "ì‘ì„±í•  ë•Œ ì£¼ì˜í•  ì ì€ ë­”ê°€ìš”?"]


def ask_with_context(message: str, history: list[dict] = []) -> dict:
    load_cache()
    clean_translation_cache()

    index_name = classify_query(message)

    # 1. query ì˜ì–´ ë³€í™˜ (GRIë§Œ)
    query = translate_to_english(message) if index_name == "GRI_Standards" else message

    print(f"âœ… ì„ íƒëœ ë²¡í„°ìŠ¤í† ì–´ index_name: {index_name}")
    vectorstore = load_vectorstore(index_name)

    # 2. ì§€í‘œ ì½”ë“œ ì¶”ì¶œ
    gri_codes = re.findall(r"\d{3}-\d+|\d{3}", message)
    code_match = gri_codes[0] if gri_codes else None

    # 3. GRI + ì½”ë“œê°€ ìˆëŠ” ê²½ìš° â†’ ì „ì²´ ë¬¸ì„œ ì¤‘ í•´ë‹¹ ì§€í‘œ í¬í•¨ëœ ì²­í¬ë§Œ í•„í„°ë§
    if index_name == "GRI_Standards" and code_match:
        query_for_filter = code_match if code_match else query
        all_docs = vectorstore.similarity_search(query_for_filter, k=100)
        docs = [d for d in all_docs if code_match in d.page_content]
        if not docs:
            print(f"âŒ ì§€í‘œ '{code_match}'ê°€ í¬í•¨ëœ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {
                "answer": f"í•´ë‹¹ ë¬¸ì„œì—ì„œ '{code_match}'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "source": index_name,
                "metadata": {},
                "table_html": ""
            }
        print(f"ğŸ¯ '{code_match}'ê°€ í¬í•¨ëœ ì²­í¬ ê°œìˆ˜: {len(docs)}")
    else:
        retriever = vectorstore.as_retriever()
        docs = retriever.get_relevant_documents(query)
        
    print("ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ì œëª© ëª©ë¡:")
    for i, d in enumerate(docs):
        title = d.metadata.get("title", "N/A")
        chunk_id = d.metadata.get("chunk_id", f"#{i}")
        print(f"  [{i}] {chunk_id} - {title}")

    # 4. context ìƒì„±
    main_title = docs[0].metadata.get("title", "")
    same_title_docs = [d for d in docs if d.metadata.get("title", "") == main_title]
    context = "\n\n".join([d.page_content for d in same_title_docs])
    print("ğŸ“¦ ì„ íƒëœ context ë¬¸ì„œ ê°œìˆ˜:", len(same_title_docs))
    for i, doc in enumerate(same_title_docs):
        print(f"--- ë¬¸ì„œ {i+1} ---")
        print("ğŸ“Œ title:", doc.metadata.get("title"))
        print("ğŸ“„ pages:", doc.metadata.get("pages"))
        print("ğŸ“ tables:", doc.metadata.get("tables"))
        print("ğŸ–¼ï¸ images:", doc.metadata.get("images"))
    print("ğŸ“š context preview:")
    print(context[:500])
    print("........")

    # 5. hallucination í•„í„° (GRI ì§€í‘œê°€ ì§ˆë¬¸ì— í¬í•¨ëëŠ”ë° contextì— ì—†ìœ¼ë©´ ì°¨ë‹¨)
    if index_name == "GRI_Standards" and gri_codes:
        asked_codes = set(re.findall(r"\d{3}-\d+", message))
        context_codes = set(re.findall(r"\d{3}-\d+", context))
        hallucinated = not asked_codes.issubset(context_codes)

        if hallucinated:
            print("âŒ GPT ì‘ë‹µ í•„í„°ë§: ì§ˆë¬¸í•œ GRI ì§€í‘œê°€ contextì— ì—†ìŒ â†’ hallucination ê°€ëŠ¥ì„±")
            return {
                "answer": "í•´ë‹¹ ë¬¸ì„œì— ìš”ì²­í•˜ì‹  GRI ì§€í‘œì— ëŒ€í•œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.",
                "source": index_name,
                "metadata": {},
                "table_html": ""
            }
    # 6. table/image ë¶„ì„
    want_table = any(k in message.lower() for k in ["í‘œ", "í…Œì´ë¸”", "table"])
    want_image = any(k in message.lower() for k in ["ì´ë¯¸ì§€", "ì‚¬ì§„", "image", "ê·¸ë¦¼"])
    metadata = {"tables": [], "images": []}
    table_html = ""

    # 5. GPTì—ê²Œ ë©”ì‹œì§€ ì „ë‹¬
    if index_name == "GRI_Standards":
        format_instruction = """
    ğŸ“ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ë”°ë¼ ì‘ë‹µí•˜ì„¸ìš”:

    ğŸ“Œ GRI {ì§€í‘œë²ˆí˜¸} ({ì§€í‘œ ì œëª©})

    1. ë¬¸ì„œì— í¬í•¨ëœ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  ì •ë³´ë¥¼ ëˆ„ë½ ì—†ì´ ìš”ì•½í•´ ì£¼ì„¸ìš”.
    - **ìš”ì•½ì€ ì§§ê²Œ í•˜ì§€ ë§ê³ **, í•­ëª©ë³„ë¡œ **ì¶©ë¶„í•œ ì„¤ëª…**ì„ í¬í•¨í•˜ì„¸ìš”.
    - **contextì— ë“±ì¥í•œ í•­ëª©ì€ ëª¨ë‘ í¬í•¨**í•˜ê³ , ë¬¸ë‹¨ êµ¬ì¡°ë¥¼ ìœ ì§€í•´ ì •ë¦¬í•˜ì„¸ìš”.
    - **ì ˆëŒ€ ì¶”ë¡ í•˜ì§€ ë§ê³ **, ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

    2. ì•„ë˜ëŠ” ì¶œë ¥ ì˜ˆì‹œì…ë‹ˆë‹¤ (í˜•ì‹ ì°¸ê³ ìš©ì´ë©° ë‚´ìš©ì€ ì§€í‘œë§ˆë‹¤ ë‹¤ë¦…ë‹ˆë‹¤):
    ğŸ“Œ GRI 302-1 (ì¡°ì§ ë‚´ ì—ë„ˆì§€ ì†Œë¹„)

    1. ë³´ê³  ì¡°ì§ì€ ë‹¤ìŒ ì •ë³´ë¥¼ ë³´ê³ í•´ì•¼ í•©ë‹ˆë‹¤:
    - ë¹„ì¬ìƒ/ì¬ìƒ ìì›ìœ¼ë¡œë¶€í„° ì†Œë¹„ëœ ì—°ë£Œ ì´ëŸ‰
    - ì „ê¸°/ë‚œë°©/ëƒ‰ë°©/ì¦ê¸° ì†Œë¹„ ë° íŒë§¤ëŸ‰
    - ì—ë„ˆì§€ ì†Œë¹„ ê³„ì‚° ê¸°ì¤€ ë° ë³€í™˜ ê³„ìˆ˜

    2. ì‘ì„± ì§€ì¹¨:
    - ì¬ìƒ/ë¹„ì¬ìƒ ì—°ë£Œë¥¼ êµ¬ë¶„í•´ ë³´ê³ 

    3. ê¶Œì¥ì‚¬í•­:
    - ë³€í™˜ ê³„ìˆ˜ ì¼ê´€ ì ìš©, ë¡œì»¬/ì¼ë°˜ ê³„ìˆ˜ ìš°ì„ ìˆœìœ„ ëª…ì‹œ

    """
        
    elif index_name == "esg_sample1":
        format_instruction = """
    ğŸ¢ ê¸°ì—…ëª…(ë˜ëŠ” ì‚°ì—…ëª…): {ê¸°ì—… ì´ë¦„ì´ë‚˜ ì‚°ì—…ì´ë¦„}
    ğŸ“Œ ESG í™œë™

    - {ë‚´ìš© ì„¤ëª…}

    â€» ì‚¬ìš©ìê°€ ì°¸ê³ í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì  ì‚¬ë¡€ ìœ„ì£¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
    """
    elif index_name == "esg_Manual":
        format_instruction = """
    ğŸ“ {ì§ˆë¬¸ì— ëŒ€í•œ ë‘ê´„ì‹ ë‹µë³€}

    - {ê´€ë ¨ ì„¤ëª…}

    - {ê´€ë ¨ ì„¤ëª…}
    ...

    """
    else:
        format_instruction = ""  # ì˜ˆì™¸ ì—†ì´ fallback

    messages = [
        SystemMessage(content=f"""
        ë„ˆëŠ” ì¤‘ì†Œê¸°ì—… ESG ë³´ê³ ì„œ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.
        ì‚¬ìš©ìëŠ” GRI ì›ë¬¸, ë§¤ë‰´ì–¼, í…œí”Œë¦¿, ì˜ˆì‹œ ë“±ì˜ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ë¬´ ì¤‘ì‹¬ì˜ ì§ˆë¬¸ì„ í•˜ê³  ìˆì–´.

        ---

        ğŸ“Œ ë„ˆì˜ ì—­í• :
        ì•„ë˜ ì œê³µë˜ëŠ” ë¬¸ì„œ(context)ì— ê¸°ë°˜í•´ ë‹¤ìŒ ì—¬ì„¯ ê°€ì§€ ì§ˆë¬¸ ìœ í˜•ì— ì ì ˆí•˜ê²Œ ì‘ë‹µí•´.

        [í™œë™ â†’ GRI ì§€í‘œ ì°¾ê¸°]
        ì‚¬ìš©ì í™œë™ì´ ì–´ë–¤ GRI ì§€í‘œì— í•´ë‹¹í•˜ëŠ”ì§€ ì„¤ëª…
        GRI ë²ˆí˜¸ì™€ í•­ëª©ëª…, ê°„ë‹¨í•œ ì •ì˜ë¥¼ ì œê³µ
        ğŸ” ë°ì´í„° ì¶œì²˜: esg_Manual

        [GRI ì§€í‘œ í•´ì„ / ì‘ì„± ë°©ë²•]
        íŠ¹ì • GRI ë²ˆí˜¸ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€, ì–´ë–¤ ë‚´ìš©ì„ ì‘ì„±í•´ì•¼ í•˜ëŠ”ì§€ ì„¤ëª…
        ğŸ” ë°ì´í„° ì¶œì²˜: esg_Manual

        [ì‘ì„± ì˜ˆì‹œ ìš”ì²­]
        í‘œê°€ ì¡´ì¬í•˜ë©´ "í‘œë¥¼ ì•„ë˜ì— ë³´ì—¬ë“œë¦´ê²Œìš”."ë¼ê³  ì‘ë‹µ
        <h3>, <table> íƒœê·¸ëŠ” ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°ë§Œ ì‚¬ìš© (ì§ì ‘ ìƒì„± ê¸ˆì§€)
        ğŸ” ë°ì´í„° ì¶œì²˜: esg_Manual

        [ë‹¤ë¥¸ ê¸°ì—… ì‚¬ë¡€ ìš”ì²­]
        ì¤‘ì†Œê¸°ì—…ì˜ ì‹¤ì œ ESG í™œë™ ì‚¬ë¡€ë¥¼ ìì„¸íˆ ì„¤ëª…
        ğŸ” ë°ì´í„° ì¶œì²˜: esg_sample1

        [ESG ê·œì • ì´ˆì•ˆ ìš”ì²­]
        í…œí”Œë¦¿ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•œ ë¬¸ì¥í˜• ì´ˆì•ˆ ì‘ì„±
        ğŸ” ë°ì´í„° ì¶œì²˜: esg_templates

        [GRI **ì›ë¬¸** ìš”ì²­]
        GRI ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì¸ìš©
        ğŸ” ë°ì´í„° ì¶œì²˜: GRI_Standards
        
        [ë§¤ë‰´ì–¼ ë‚´ìš© ì„¤ëª…]
        Context ë‚´ì˜ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œ ìš”ì•½
        ğŸ” ë°ì´í„° ì¶œì²˜: esg_Manual

        ---

        ğŸ“Œ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì‘ë‹µ ê·œì¹™:

        ë¬¸ì„œ(context)ì— ë“±ì¥í•˜ì§€ ì•ŠëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
        ì¶”ë¡ , ìƒì‹, GPT í›ˆë ¨ ì •ë³´ ì‚¬ìš© ê¸ˆì§€

        ì‘ë‹µì€ í•­ìƒ ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.
        ì˜ˆ: â€œGRI 302-1ì€ ì—ë„ˆì§€ ì‚¬ìš©ì„ ë³´ê³ í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤.â€
        ê·¸ í›„, ë¬¸ì„œ(context)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ ì„¤ëª…, ì‘ì„± í•­ëª©, ì •ì˜, ì˜ˆì‹œ ë“±ì„ 2~3ë¬¸ì¥ ì´ìƒ ë³´ì™„í•˜ì„¸ìš”.
        ì „ì²´ ì‘ë‹µì€ 7ë¬¸ì¥ ì´ë‚´ë¡œ êµ¬ì„±í•˜ì„¸ìš”.

        í‘œ ìš”ì²­ì´ ìˆì„ ê²½ìš°
        ë¬¸ì„œ ê¸°ë°˜ í‘œ ìš”êµ¬ ë°›ì„ë•Œ contextì˜ ê°„ëµí•œ ì„¤ëª…ê³¼ í•¨ê»˜ "í‘œë¥¼ ì•„ë˜ì— ë³´ì—¬ë“œë¦´ê²Œìš”."ë¼ê³ ë§Œ ì‘ë‹µ
        <h3>, <table>ì€ ì‹œìŠ¤í…œì´ ë”°ë¡œ ì¶”ê°€í•˜ë¯€ë¡œ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”

        ë‹¤ìŒ í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”:
        "ì£„ì†¡í•˜ì§€ë§Œ", "í›ˆë ¨ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œëŠ”", "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "GPTëŠ”â€¦" ë“±
        ---
        ğŸ“˜ ë¬¸ì„œ(context)ëŠ” ì•„ë˜ì— ì œê³µë©ë‹ˆë‹¤.  
        â€» ë¬¸ì„œì— ê¸°ë°˜í•œ ì„¤ëª…ë§Œ í—ˆìš©ë˜ë©°, ë„¤ê°€ ë¬¸ì„œë¥¼ ì¶”ë¡ í•˜ê±°ë‚˜ ê±°ì§“ ì‘ë‹µì„ ì§€ì–´ë‚´ì„  ì•ˆ ë©ë‹ˆë‹¤.

        {context}
        ---
        ê°€ë…ì„±ì„ ìœ„í•œ ì‘ë‹µ ì–‘ì‹
        {format_instruction}

        """),
        HumanMessage(content=query)
    ]


    llm = ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=key["OPENAI_API_KEY"])
    output = llm.invoke(messages).content
    print("ğŸ” GPT ì‘ë‹µ:", output)

    # 8. table/image ì¶”ì¶œ
    if want_table or want_image:
        # âœ… ë¬¸ì„œ ì „ì²´ì—ì„œ í…Œì´ë¸”/ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ
        extracted = extract_metadata(docs)
        table_paths = extracted["tables"]
        image_paths = extracted["images"]
        table_htmls = []

        if want_table and table_paths:
            seen_tables = set()
            for table_path in table_paths:
                try:
                    # table_pathê°€ html íƒœê·¸ì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    if "<table" in table_path and "</table>" in table_path:
                        raw_html = table_path
                    else:
                        raw_html = Path(table_path).read_text(encoding="utf-8")

                    cleaned = extract_clean_table_html(raw_html)
                    if cleaned.strip() and cleaned not in seen_tables:
                        table_htmls.append(cleaned)
                        seen_tables.add(cleaned)

                except Exception as e:
                    print(f"âŒ í…Œì´ë¸” ì½ê¸° ì‹¤íŒ¨: {table_path} â†’ {e}")

        metadata = {"tables": table_paths}
        table_html = "\n<hr/>\n".join(table_htmls)

        print(f"ğŸ“¥ í‘œ/ì´ë¯¸ì§€ ìš”ì²­ ê°ì§€ë¨ â†’ table={want_table}, image={want_image}")
        print(f"ğŸ“ table paths: {table_paths}")
        print(f"ğŸ–¼ image list: {image_paths}")

    # 9. ë²ˆì—­ ì—¬ë¶€
    final_answer = translate_to_korean(output) if index_name == "GRI_Standards" else output

    save_cache()
    return {
        "answer": final_answer,
        "source": index_name,
        "metadata": metadata,
        "table_html": table_html,
        "suggested_questions": generate_suggested_questions(context, message, index_name)
    }
